
# ESMS‑VAE

<p align="center">
  <a href="https://doi.org/10.1093/bioinformatics/btzXXX"><img src="https://img.shields.io/badge/Paper-Bioinformatics(TMD)-green.svg?style=flat-square" alt="paper"></a>
  <a href="https://github.com/Ahnd6474/ESMS-VAE/blob/main/LICENSE"><img src="https://img.shields.io/github/license/Ahnd6474/ESMS-VAE?style=flat-square" alt="license"></a>
  <a href="#"><img src="https://img.shields.io/badge/python-3.9%2B-blue.svg?style=flat-square"></a>
  <a href="#"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square"></a>
</p>

> **ESMS‑VAE** (*Evolutionary Scale Modeling Student VAE*) is a 5.5 M‑parameter transformer VAE that learns structure‑aware latent representations of proteins through a novel **structural loss**.  It reaches **97.17 %** reconstruction accuracy on UniRef50 sequences and surpasses prior VAEs on the ProteinGym benchmark (*ρ = 0.689*).  Downstream tasks such as fluorescent‑protein classification (F1 = 0.99) and wavelength regression (RMSE ≈ 3 nm) confirm its practical utility.

---

**Table of Contents**

1. [Features](#features)
2. [Method](#method)
3. [Installation](#installation)
4. [Quick Start](#quick-start)
5. [Repository Structure](#repository-structure)
6. [Using the Notebooks](#using-the-notebooks)
7. [Pre‑trained Models](#pre-trained-models)
8. [Reproducing Paper Results](#reproducing-paper-results)
9. [Benchmarks](#benchmarks)
10. [Citation](#citation)
11. [Availability and implementation](#Availability-and-implementation)
12. [License](#license)
13. [Contact](#Contact)
---

### Features

- **ESMS student encoder** – a 6‑layer, 256‑dim model **distilled from ESM‑2** (650 M params) while preserving ≥ 99 % cosine‑similarity
- **Structure‑aware learning** – latent vectors are explicitly aligned to ESMS embeddings via **cosine + MSE loss**, enabling geometry‑savvy generation without 3‑D supervision.
- **Lightweight** – 5.5 M parameters; end‑to‑end training on a double T4 in ≤ 6 h.
- **High fidelity** – 97 % sequence‑level reconstruction on UniRef50 test split.
- **Robust latent space** – maintains active KL (~0.05) and avoids posterior collapse.
- **Broad generalisation** – tops Kermut on all 162/217 ProteinGym DMS sets (ρ = 0.78/0.69).
- **Plug‑and‑play embeddings** – drop‑in replacement for UniRep/ESM features in downstream GP/NN models.

---

### Method

The training objective combines structure alignment, reconstruction, classification, and KL regularisation:

```math
L = \lambda\,(L_{\text{MSE}} + L_{\text{COS}})\; +\; \alpha\,L_{\text{CE}}\; +\; \beta\,L_{\text{KL}}
```

where

* **L<sub>COS</sub>** = 1 − cos(ESMS(**origin**), ESMS(**recon**))
* **L<sub>MSE</sub>** = ∥ESMS(**origin**) − ESMS(**recon**)∥²

See *docs/ESMS_VAE.pdf* (Eq. 3, L90‑L101) for details.

---

### Installation

```bash
# 1. Clone
git clone https://github.com/Ahnd6474/ESMS-VAE.git
cd ESMS-VAE

# 2. Create env (optional)
conda create -n esms-vae python=3.9 -y
conda activate esms-vae

# 3. Install Python requirements
pip install -r requirements.txt
```

> **GPU:** A single NVIDIA T4/RX‑A5000 (~16 GB) is sufficient for both training and inference.

---

### Quick Start

```python
from vae_module import Tokenizer, Config, load_vae, encode, decode

cfg = Config(model_path="models/vae_epoch380.pt")
tok = Tokenizer.from_esm()

model = load_vae(cfg,
                 vocab_size=len(tok.vocab),
                 pad_idx=tok.pad_idx,
                 bos_idx=tok.bos_idx)

seq = "MKTFFVLLLACTIVCLLA"
z   = encode(model, seq, tok, cfg.max_len)
new_seq = decode(model, z, tok)
print(new_seq)
```

---

### Repository Structure

- `notebooks/esms-vae-structured.ipynb` – step‑by‑step training and evaluation workflow.
- `notebooks/gfp-cluster.ipynb` – explores latent space with **KMeans** clustering.
- `notebooks/gfp-regressor.ipynb` – fits a regression model to latent features.
- `models/vae_epoch380.pt` – pretrained checkpoint produced by the training notebook.
- `docs/ESMS_VAE.pdf` – short paper summarising the method (also referenced below).

---

### Using the Notebooks

After training, you can explore the latent space further:

1. **gfp-cluster.ipynb** – cluster sequences using KMeans on latent vectors.
2. **gfp-regressor.ipynb** – fit a simple regressor (e.g., Ridge/XGBoost) on latent features to predict fluorescence properties.

Both notebooks assume latent vectors have been generated by **esms‑vae‑structured.ipynb**.

---

### Pre‑trained Models

| File              | Epoch | KL    | Rec. Acc.   | Notes                                  |
| ----------------- | ----- | ----- | ----------- | -------------------------------------- |
| `vae_epoch380.pt` | 380   | 0.048 | **97.17 %** | Paper model (used in all experiments)  |
| `vae_epoch500.pt` | 500   | 0.002 | 99.98 %     | High accuracy but suffers KL vanishing |

These model files are tracked with **Git Large File Storage (LFS)**.
After cloning the repository, run:

```bash
git lfs pull
```

to download the checkpoints into `models/`.

---

### Reproducing Paper Results

```bash
# Training on UniRef50 subset
python train_baseline.py --data data/uniref50_subsample.fasta \
                         --epochs 380 \
                         --save models/vae_epoch380.pt

# ProteinGym inference (takes ≈3 h)
python protein_gym_evaluate.py --weights models/vae_epoch380.pt
```

The scripts will output a CSV matching Table S2 of the paper.

---

### Benchmarks

| Task              | Dataset          | Metric     | ESMS‑VAE   | Previous SOTA    |
| ----------------- | ---------------- | ---------- | ---------- | ---------------- |
| Reconstruction    | UniRef50 test    | % accurate | **97.17**  |N/A|
| Mutational effect | ProteinGym (162/217) | Spearman ρ | **0.7779/0.689** |0.698/0.657|
| FP vs non‑FP      | FPbase           | 5‑fold Acc | **0.987**  |N/A|
| λabs              | FPbase           | RMSE (nm)  | **2.70**   |N/A|
| λem               | FPbase           | RMSE (nm)  | **3.80**   |N/A|

---
### Limitations
Maximum sequence length 512. The model and tokeniser were configured with max_len = 512 to fit on a double-GPU setup. For proteins longer than 512 residues we split each sequence into non‑overlapping 512‑aa chunks before encoding/decoding. This chunking hurts mutational‑effect performance: on the 55 ProteinGym datasets containing sequences > 512 aa the Spearman correlation drops to 0.427, whereas on the other 162 datasets (≤ 512 aa) the model reaches 0.7779.
### Citation

If you use this code, please cite:

```bibtex
@article{ahn2025esmsvae,
  title={ESMS VAE: A Structure-Informed Variational Autoencoder for Protein Engineering},
  author={Ahn, Danny and Lee, Minjae and Moon, Shihyun and Jung, Jooyoung},
  journal={Bioinformatics},
  year={2025},
  doi={10.1093/bioinformatics/btzXXX}
}
```
### Availability and implementation

ESMS‑VAE (source code and pre‑trained models) is **freely available for non‑commercial research and educational use** at this GitHub repository under the *ESMS‑VAE Non‑Commercial Research License v1.1*.  
**Commercial use requires prior written permission from the investor “안성톱밥”.**

A snapshot of the exact version used in the manuscript, together with model checkpoints and test data, will be archived on Zenodo at the time of submission and referenced by a DOI.

---

### License

The code and pre‑trained models are licensed for **non‑commercial research use only**.  
See the accompanying [LICENSE](LICENSE) file for the full terms.

## License

This project is licensed under the **[Your Project Name] Non-Commercial License**.  
See `LICENSE` for details.

### Third‑Party Components

| Component     | License                      | License Text                          |
|--------------|-----------------------------|---------------------------------------|
| scikit‑learn | BSD 3‑Clause                | [third_party/licenses/scikit‑learn-BSD-3-Clause.txt] |
| PyTorch      | BSD 3‑Clause (Modified BSD) | [third_party/licenses/pytorch-BSD-3-Clause.txt]      |
| fpbase       | GPL 3.0                     | [third_party/licenses/fpbase-GPL-3.0.txt]            |
| esm‑2        | MIT                         | [third_party/licenses/esm-2-MIT.txt]                |
| biopython    | Biopython License           | [third_party/licenses/Biopython_LICENSE.rst]         |
| tqdm         | MPL 2.0                     | [third_party/licenses/tqdm-MPL-2.0.txt]             |
| UniRef50     | CC BY 4.0                   | https://creativecommons.org/licenses/by/4.0/        |


### Contact
<p>Contact: <a href="mailto:ahnd6474@gmail.com">ahnd6474@gmail.com</a></p>
---
